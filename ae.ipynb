{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Autoencoder experiment\n",
    "\n",
    "1. Build an autoencoder and train it on train data (MIT's)\n",
    "2. Save the encoded train data (MIT's)\n",
    "3. Take encoder part from the AE and encode youtube data (the data that will work as \"augmenter\")\n",
    "4. Apply some clustering or other algorithm to find similar segments in both encoded datasets (using some MIT videos as seeds)\n",
    "5. Train NN on the MIT and YT data (those segments that are similar to MIT)\n",
    "6. Validate the classifier (so you may need to leave some of the MIT and YT data out of the training)\n",
    "7. Test on our own data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/media/adilkhan/HDD/Anaconda3/lib/python3.6/site-packages/librosa/filters.py:261: UserWarning: Empty filters detected in mel frequency basis. Some channels will produce empty responses. Try increasing your sampling rate (and fmax) or reducing n_mels.\n",
      "  warnings.warn('Empty filters detected in mel frequency basis. '\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pickle\n",
    "from scipy.io import wavfile\n",
    "import numpy as np\n",
    "from librosa import feature\n",
    "from keras.utils import to_categorical\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import minmax_scale\n",
    "\n",
    "\n",
    "def extract(filename):\n",
    "    rate, frames = wavfile.read(filename)\n",
    "    window = 4096\n",
    "    nfft = 64\n",
    "    feat = []\n",
    "    # len(frames)-window\n",
    "    for i in range(0, len(frames) - window, window):\n",
    "        # feat.append(np.array(frames[i:i + window]))\n",
    "        feat.append(np.array(feature.mfcc(frames[i:i + window - 1],\n",
    "                                   sr=rate,\n",
    "                                   n_fft=nfft,\n",
    "                                   hop_length=round(nfft / 2),\n",
    "                                   fmax=8000)).flatten())\n",
    "    feat = np.stack(feat)\n",
    "    return feat\n",
    "    \n",
    "def extract_features(file_wet, file_dry):\n",
    "    to_replace =\"\\\\/\"\n",
    "    for char in to_replace:\n",
    "        fw = file_wet.replace(char, \"_\")\n",
    "        fd = file_dry.replace(char, \"_\")\n",
    "    pickle_file = fw + \"-\" + fd + \".pkl\"\n",
    "    if os.path.exists(pickle_file):\n",
    "        print(\"Using pickle file\", pickle_file)\n",
    "        with open(pickle_file, \"rb\") as f:\n",
    "            features, labels = pickle.load(f)\n",
    "        return features, labels\n",
    "    features_wet = extract(file_wet)\n",
    "    features_dry = extract(file_dry)\n",
    "    labels_wet = np.ones(features_wet.shape[0])\n",
    "    labels_dry = np.zeros(features_dry.shape[0])\n",
    "    features = np.concatenate((features_wet, features_dry))\n",
    "    labels = np.concatenate((labels_wet, labels_dry))\n",
    "    labels = to_categorical(labels)\n",
    "    features = minmax_scale(features)\n",
    "    with open(pickle_file, \"wb\") as f:\n",
    "        pickle.dump((features, labels), f, protocol=4)\n",
    "    return features, labels\n",
    "    \n",
    "X, y = extract_features(\"yt_data/wet_mono.wav\", \"yt_data/dry_mono.wav\")\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0000000000000002"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         (None, 2560)              0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 1000)              2561000   \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 500)               500500    \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 200)               100200    \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 500)               100500    \n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              (None, 1000)              501000    \n",
      "_________________________________________________________________\n",
      "dense_6 (Dense)              (None, 2560)              2562560   \n",
      "=================================================================\n",
      "Total params: 6,325,760\n",
      "Trainable params: 6,325,760\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "from keras.layers import Input, Dense\n",
    "from keras.models import Model\n",
    "\n",
    "# ae = models.Sequential()\n",
    "#encoder\n",
    "# d1 = Dense(1000, activation='tanh',input_shape=(X_train.shape[1:]))\n",
    "# ae.add(d1)\n",
    "# ae.add(BatchNormalization())\n",
    "# #ae.add(Dropout(0.8))\n",
    "# d2 = Dense(500, activation='tanh')\n",
    "# ae.add(d2)\n",
    "# #ae.add(Dropout(0.8))\n",
    "# #decoder\n",
    "# ae.add(Dense(d2.input_shape[1], activation='tanh'))\n",
    "# ae.add(Dense(d1.input_shape[1], activation='tanh'))\n",
    "\n",
    "inp = Input(shape=(X_train.shape[1:]))\n",
    "encoded = Dense(1000, activation='relu')(inp)\n",
    "encoded = Dense(500, activation='relu')(encoded)\n",
    "encoded = Dense(200, activation='relu')(encoded)\n",
    "\n",
    "decoded = Dense(500, activation='relu')(encoded)\n",
    "decoded = Dense(1000, activation='relu')(decoded)\n",
    "decoded = Dense(X_train.shape[1], activation='sigmoid')(decoded)\n",
    "ae = Model(inp, decoded)\n",
    "ae.compile(loss='mse',\n",
    "optimizer='adadelta', metrics=['accuracy'])\n",
    "ae.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 467834 samples, validate on 230427 samples\n",
      "Epoch 1/100\n",
      "467834/467834 [==============================] - 38s - loss: 0.0082 - acc: 0.0134 - val_loss: 0.0067 - val_acc: 0.0070\n",
      "Epoch 2/100\n",
      "467834/467834 [==============================] - 32s - loss: 0.0066 - acc: 0.0094 - val_loss: 0.0066 - val_acc: 0.0143\n",
      "Epoch 3/100\n",
      "467834/467834 [==============================] - 32s - loss: 0.0066 - acc: 0.0213 - val_loss: 0.0066 - val_acc: 0.0226\n",
      "Epoch 4/100\n",
      "467834/467834 [==============================] - 32s - loss: 0.0066 - acc: 0.0281 - val_loss: 0.0066 - val_acc: 0.0266\n",
      "Epoch 5/100\n",
      "467834/467834 [==============================] - 34s - loss: 0.0065 - acc: 0.0234 - val_loss: 0.0065 - val_acc: 0.0178\n",
      "Epoch 6/100\n",
      "467834/467834 [==============================] - 32s - loss: 0.0064 - acc: 0.0173 - val_loss: 0.0064 - val_acc: 0.0177\n",
      "Epoch 7/100\n",
      "467834/467834 [==============================] - 32s - loss: 0.0064 - acc: 0.0166 - val_loss: 0.0064 - val_acc: 0.0171\n",
      "Epoch 8/100\n",
      "467834/467834 [==============================] - 32s - loss: 0.0063 - acc: 0.0168 - val_loss: 0.0063 - val_acc: 0.0165\n",
      "Epoch 9/100\n",
      "467834/467834 [==============================] - 33s - loss: 0.0063 - acc: 0.0169 - val_loss: 0.0063 - val_acc: 0.0166\n",
      "Epoch 10/100\n",
      "467834/467834 [==============================] - 32s - loss: 0.0063 - acc: 0.0169 - val_loss: 0.0063 - val_acc: 0.0172\n",
      "Epoch 11/100\n",
      "467834/467834 [==============================] - 34s - loss: 0.0063 - acc: 0.0175 - val_loss: 0.0063 - val_acc: 0.0194\n",
      "Epoch 12/100\n",
      "467834/467834 [==============================] - 32s - loss: 0.0062 - acc: 0.0185 - val_loss: 0.0062 - val_acc: 0.0195\n",
      "Epoch 13/100\n",
      "467834/467834 [==============================] - 33s - loss: 0.0062 - acc: 0.0197 - val_loss: 0.0062 - val_acc: 0.0210\n",
      "Epoch 14/100\n",
      "467834/467834 [==============================] - 32s - loss: 0.0062 - acc: 0.0209 - val_loss: 0.0062 - val_acc: 0.0228\n",
      "Epoch 15/100\n",
      "467834/467834 [==============================] - 32s - loss: 0.0062 - acc: 0.0221 - val_loss: 0.0062 - val_acc: 0.0223\n",
      "Epoch 16/100\n",
      "467834/467834 [==============================] - 32s - loss: 0.0062 - acc: 0.0235 - val_loss: 0.0062 - val_acc: 0.0226\n",
      "Epoch 17/100\n",
      "467834/467834 [==============================] - 33s - loss: 0.0062 - acc: 0.0240 - val_loss: 0.0062 - val_acc: 0.0256\n",
      "Epoch 18/100\n",
      "467834/467834 [==============================] - 32s - loss: 0.0062 - acc: 0.0249 - val_loss: 0.0062 - val_acc: 0.0266\n",
      "Epoch 19/100\n",
      "467834/467834 [==============================] - 32s - loss: 0.0062 - acc: 0.0254 - val_loss: 0.0062 - val_acc: 0.0275\n",
      "Epoch 20/100\n",
      "467834/467834 [==============================] - 32s - loss: 0.0062 - acc: 0.0256 - val_loss: 0.0062 - val_acc: 0.0254\n",
      "Epoch 21/100\n",
      "467834/467834 [==============================] - 33s - loss: 0.0062 - acc: 0.0260 - val_loss: 0.0062 - val_acc: 0.0251\n",
      "Epoch 22/100\n",
      "467834/467834 [==============================] - 32s - loss: 0.0062 - acc: 0.0260 - val_loss: 0.0062 - val_acc: 0.0271\n",
      "Epoch 23/100\n",
      "467834/467834 [==============================] - 33s - loss: 0.0062 - acc: 0.0261 - val_loss: 0.0062 - val_acc: 0.0263\n",
      "Epoch 24/100\n",
      "467834/467834 [==============================] - 32s - loss: 0.0061 - acc: 0.0261 - val_loss: 0.0061 - val_acc: 0.0268\n",
      "Epoch 25/100\n",
      "467834/467834 [==============================] - 32s - loss: 0.0061 - acc: 0.0265 - val_loss: 0.0061 - val_acc: 0.0259\n",
      "Epoch 26/100\n",
      "467834/467834 [==============================] - 32s - loss: 0.0061 - acc: 0.0265 - val_loss: 0.0061 - val_acc: 0.0260\n",
      "Epoch 27/100\n",
      "467834/467834 [==============================] - 32s - loss: 0.0061 - acc: 0.0266 - val_loss: 0.0061 - val_acc: 0.0258\n",
      "Epoch 28/100\n",
      "467834/467834 [==============================] - 31s - loss: 0.0061 - acc: 0.0266 - val_loss: 0.0061 - val_acc: 0.0267\n",
      "Epoch 29/100\n",
      "467834/467834 [==============================] - 33s - loss: 0.0061 - acc: 0.0266 - val_loss: 0.0061 - val_acc: 0.0265\n",
      "Epoch 30/100\n",
      "467834/467834 [==============================] - 32s - loss: 0.0061 - acc: 0.0265 - val_loss: 0.0061 - val_acc: 0.0258\n",
      "Epoch 31/100\n",
      "467834/467834 [==============================] - 33s - loss: 0.0061 - acc: 0.0264 - val_loss: 0.0061 - val_acc: 0.0255\n",
      "Epoch 32/100\n",
      "467834/467834 [==============================] - 32s - loss: 0.0061 - acc: 0.0259 - val_loss: 0.0061 - val_acc: 0.0262\n",
      "Epoch 33/100\n",
      "467834/467834 [==============================] - 34s - loss: 0.0061 - acc: 0.0259 - val_loss: 0.0061 - val_acc: 0.0251\n",
      "Epoch 34/100\n",
      "467834/467834 [==============================] - 32s - loss: 0.0061 - acc: 0.0254 - val_loss: 0.0061 - val_acc: 0.0253\n",
      "Epoch 35/100\n",
      "467834/467834 [==============================] - 32s - loss: 0.0061 - acc: 0.0247 - val_loss: 0.0061 - val_acc: 0.0242\n",
      "Epoch 36/100\n",
      "467834/467834 [==============================] - 32s - loss: 0.0061 - acc: 0.0242 - val_loss: 0.0061 - val_acc: 0.0245\n",
      "Epoch 37/100\n",
      "467834/467834 [==============================] - 33s - loss: 0.0061 - acc: 0.0235 - val_loss: 0.0061 - val_acc: 0.0240\n",
      "Epoch 38/100\n",
      "467834/467834 [==============================] - 32s - loss: 0.0061 - acc: 0.0227 - val_loss: 0.0061 - val_acc: 0.0228\n",
      "Epoch 39/100\n",
      "467834/467834 [==============================] - 33s - loss: 0.0061 - acc: 0.0223 - val_loss: 0.0061 - val_acc: 0.0227\n",
      "Epoch 40/100\n",
      "467834/467834 [==============================] - 32s - loss: 0.0061 - acc: 0.0218 - val_loss: 0.0061 - val_acc: 0.0226\n",
      "Epoch 41/100\n",
      "467834/467834 [==============================] - 32s - loss: 0.0061 - acc: 0.0212 - val_loss: 0.0061 - val_acc: 0.0215\n",
      "Epoch 42/100\n",
      "467834/467834 [==============================] - 32s - loss: 0.0061 - acc: 0.0211 - val_loss: 0.0061 - val_acc: 0.0201\n",
      "Epoch 43/100\n",
      "467834/467834 [==============================] - 32s - loss: 0.0061 - acc: 0.0205 - val_loss: 0.0061 - val_acc: 0.0208\n",
      "Epoch 44/100\n",
      "467834/467834 [==============================] - 33s - loss: 0.0060 - acc: 0.0203 - val_loss: 0.0060 - val_acc: 0.0204\n",
      "Epoch 45/100\n",
      "467834/467834 [==============================] - 32s - loss: 0.0060 - acc: 0.0202 - val_loss: 0.0060 - val_acc: 0.0202\n",
      "Epoch 46/100\n",
      "467834/467834 [==============================] - 32s - loss: 0.0060 - acc: 0.0200 - val_loss: 0.0060 - val_acc: 0.0202\n",
      "Epoch 47/100\n",
      "467834/467834 [==============================] - 32s - loss: 0.0060 - acc: 0.0198 - val_loss: 0.0060 - val_acc: 0.0197\n",
      "Epoch 48/100\n",
      "467834/467834 [==============================] - 31s - loss: 0.0060 - acc: 0.0197 - val_loss: 0.0060 - val_acc: 0.0190\n",
      "Epoch 49/100\n",
      "467834/467834 [==============================] - 33s - loss: 0.0060 - acc: 0.0197 - val_loss: 0.0060 - val_acc: 0.0195\n",
      "Epoch 50/100\n",
      "467834/467834 [==============================] - 32s - loss: 0.0060 - acc: 0.0196 - val_loss: 0.0060 - val_acc: 0.0190\n",
      "Epoch 51/100\n",
      "467834/467834 [==============================] - 32s - loss: 0.0060 - acc: 0.0195 - val_loss: 0.0060 - val_acc: 0.0200\n",
      "Epoch 52/100\n",
      "467834/467834 [==============================] - 32s - loss: 0.0060 - acc: 0.0193 - val_loss: 0.0060 - val_acc: 0.0199\n",
      "Epoch 53/100\n",
      "467834/467834 [==============================] - 33s - loss: 0.0060 - acc: 0.0193 - val_loss: 0.0060 - val_acc: 0.0189\n",
      "Epoch 54/100\n",
      "467834/467834 [==============================] - 33s - loss: 0.0060 - acc: 0.0191 - val_loss: 0.0060 - val_acc: 0.0194\n",
      "Epoch 55/100\n",
      "467834/467834 [==============================] - 33s - loss: 0.0060 - acc: 0.0190 - val_loss: 0.0060 - val_acc: 0.0181\n",
      "Epoch 56/100\n",
      "467834/467834 [==============================] - 32s - loss: 0.0060 - acc: 0.0189 - val_loss: 0.0060 - val_acc: 0.0192\n",
      "Epoch 57/100\n",
      "467834/467834 [==============================] - 32s - loss: 0.0060 - acc: 0.0189 - val_loss: 0.0060 - val_acc: 0.0187\n",
      "Epoch 58/100\n",
      "467834/467834 [==============================] - 32s - loss: 0.0060 - acc: 0.0188 - val_loss: 0.0060 - val_acc: 0.0184\n",
      "Epoch 59/100\n",
      "467834/467834 [==============================] - 32s - loss: 0.0060 - acc: 0.0188 - val_loss: 0.0060 - val_acc: 0.0188\n",
      "Epoch 60/100\n",
      "467834/467834 [==============================] - 31s - loss: 0.0060 - acc: 0.0186 - val_loss: 0.0060 - val_acc: 0.0182\n",
      "Epoch 61/100\n",
      "467834/467834 [==============================] - 32s - loss: 0.0060 - acc: 0.0186 - val_loss: 0.0060 - val_acc: 0.0186\n",
      "Epoch 62/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "467834/467834 [==============================] - 32s - loss: 0.0059 - acc: 0.0185 - val_loss: 0.0059 - val_acc: 0.0184\n",
      "Epoch 63/100\n",
      "467834/467834 [==============================] - 32s - loss: 0.0059 - acc: 0.0185 - val_loss: 0.0059 - val_acc: 0.0187\n",
      "Epoch 64/100\n",
      "467834/467834 [==============================] - 32s - loss: 0.0059 - acc: 0.0185 - val_loss: 0.0059 - val_acc: 0.0185\n",
      "Epoch 65/100\n",
      "467834/467834 [==============================] - 32s - loss: 0.0059 - acc: 0.0184 - val_loss: 0.0059 - val_acc: 0.0188\n",
      "Epoch 66/100\n",
      "467834/467834 [==============================] - 33s - loss: 0.0059 - acc: 0.0184 - val_loss: 0.0059 - val_acc: 0.0177\n",
      "Epoch 67/100\n",
      "467834/467834 [==============================] - 33s - loss: 0.0059 - acc: 0.0184 - val_loss: 0.0059 - val_acc: 0.0188\n",
      "Epoch 68/100\n",
      "467834/467834 [==============================] - 33s - loss: 0.0059 - acc: 0.0182 - val_loss: 0.0059 - val_acc: 0.0178\n",
      "Epoch 69/100\n",
      "467834/467834 [==============================] - 32s - loss: 0.0059 - acc: 0.0182 - val_loss: 0.0059 - val_acc: 0.0182\n",
      "Epoch 70/100\n",
      "467834/467834 [==============================] - 32s - loss: 0.0059 - acc: 0.0180 - val_loss: 0.0059 - val_acc: 0.0179\n",
      "Epoch 71/100\n",
      "467834/467834 [==============================] - 32s - loss: 0.0059 - acc: 0.0181 - val_loss: 0.0059 - val_acc: 0.0183\n",
      "Epoch 72/100\n",
      "467834/467834 [==============================] - 32s - loss: 0.0059 - acc: 0.0179 - val_loss: 0.0059 - val_acc: 0.0183\n",
      "Epoch 73/100\n",
      "467834/467834 [==============================] - 33s - loss: 0.0059 - acc: 0.0179 - val_loss: 0.0059 - val_acc: 0.0175\n",
      "Epoch 74/100\n",
      "467834/467834 [==============================] - 32s - loss: 0.0059 - acc: 0.0179 - val_loss: 0.0059 - val_acc: 0.0177\n",
      "Epoch 75/100\n",
      "467834/467834 [==============================] - 33s - loss: 0.0059 - acc: 0.0178 - val_loss: 0.0059 - val_acc: 0.0177\n",
      "Epoch 76/100\n",
      "467834/467834 [==============================] - 32s - loss: 0.0059 - acc: 0.0178 - val_loss: 0.0059 - val_acc: 0.0180\n",
      "Epoch 77/100\n",
      "467834/467834 [==============================] - 33s - loss: 0.0059 - acc: 0.0177 - val_loss: 0.0059 - val_acc: 0.0178\n",
      "Epoch 78/100\n",
      "467834/467834 [==============================] - 32s - loss: 0.0059 - acc: 0.0176 - val_loss: 0.0058 - val_acc: 0.0177\n",
      "Epoch 79/100\n",
      "467834/467834 [==============================] - 33s - loss: 0.0058 - acc: 0.0176 - val_loss: 0.0058 - val_acc: 0.0177\n",
      "Epoch 80/100\n",
      "467834/467834 [==============================] - 32s - loss: 0.0058 - acc: 0.0177 - val_loss: 0.0058 - val_acc: 0.0176\n",
      "Epoch 81/100\n",
      "467834/467834 [==============================] - 33s - loss: 0.0058 - acc: 0.0176 - val_loss: 0.0058 - val_acc: 0.0174\n",
      "Epoch 82/100\n",
      "467834/467834 [==============================] - 32s - loss: 0.0058 - acc: 0.0176 - val_loss: 0.0058 - val_acc: 0.0173\n",
      "Epoch 83/100\n",
      "467834/467834 [==============================] - 33s - loss: 0.0058 - acc: 0.0175 - val_loss: 0.0058 - val_acc: 0.0169\n",
      "Epoch 84/100\n",
      "467834/467834 [==============================] - 33s - loss: 0.0058 - acc: 0.0174 - val_loss: 0.0058 - val_acc: 0.0172\n",
      "Epoch 85/100\n",
      "467834/467834 [==============================] - 34s - loss: 0.0058 - acc: 0.0174 - val_loss: 0.0058 - val_acc: 0.0176\n",
      "Epoch 86/100\n",
      "467834/467834 [==============================] - 34s - loss: 0.0058 - acc: 0.0174 - val_loss: 0.0058 - val_acc: 0.0173\n",
      "Epoch 87/100\n",
      "467834/467834 [==============================] - 35s - loss: 0.0058 - acc: 0.0173 - val_loss: 0.0058 - val_acc: 0.0172\n",
      "Epoch 88/100\n",
      "467834/467834 [==============================] - 34s - loss: 0.0058 - acc: 0.0174 - val_loss: 0.0058 - val_acc: 0.0175\n",
      "Epoch 89/100\n",
      "467834/467834 [==============================] - 34s - loss: 0.0058 - acc: 0.0173 - val_loss: 0.0058 - val_acc: 0.0169\n",
      "Epoch 90/100\n",
      "467834/467834 [==============================] - 34s - loss: 0.0058 - acc: 0.0173 - val_loss: 0.0058 - val_acc: 0.0172\n",
      "Epoch 91/100\n",
      "467834/467834 [==============================] - 34s - loss: 0.0058 - acc: 0.0174 - val_loss: 0.0058 - val_acc: 0.0171\n",
      "Epoch 92/100\n",
      "467834/467834 [==============================] - 34s - loss: 0.0058 - acc: 0.0173 - val_loss: 0.0058 - val_acc: 0.0176\n",
      "Epoch 93/100\n",
      "467834/467834 [==============================] - 35s - loss: 0.0058 - acc: 0.0173 - val_loss: 0.0058 - val_acc: 0.0173\n",
      "Epoch 94/100\n",
      "467834/467834 [==============================] - 34s - loss: 0.0058 - acc: 0.0172 - val_loss: 0.0058 - val_acc: 0.0170\n",
      "Epoch 95/100\n",
      "467834/467834 [==============================] - 34s - loss: 0.0057 - acc: 0.0172 - val_loss: 0.0057 - val_acc: 0.0171\n",
      "Epoch 96/100\n",
      "467834/467834 [==============================] - 34s - loss: 0.0057 - acc: 0.0172 - val_loss: 0.0057 - val_acc: 0.0171\n",
      "Epoch 97/100\n",
      "467834/467834 [==============================] - 35s - loss: 0.0057 - acc: 0.0171 - val_loss: 0.0057 - val_acc: 0.0174\n",
      "Epoch 98/100\n",
      "467834/467834 [==============================] - 34s - loss: 0.0057 - acc: 0.0170 - val_loss: 0.0057 - val_acc: 0.0170\n",
      "Epoch 99/100\n",
      "467834/467834 [==============================] - 34s - loss: 0.0057 - acc: 0.0170 - val_loss: 0.0057 - val_acc: 0.0170\n",
      "Epoch 100/100\n",
      "467834/467834 [==============================] - 34s - loss: 0.0057 - acc: 0.0170 - val_loss: 0.0057 - val_acc: 0.0169\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f1db9ae19e8>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ae.fit(X_train, X_train, epochs=100,\n",
    "          batch_size=128,\n",
    "          validation_data=(X_test, X_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<keras.engine.topology.InputLayer at 0x7f1db9be7cf8>,\n",
       " <keras.layers.core.Dense at 0x7f1db9be7cc0>,\n",
       " <keras.layers.core.Dense at 0x7f1db9be7ef0>,\n",
       " <keras.layers.core.Dense at 0x7f1db9674048>,\n",
       " <keras.layers.core.Dense at 0x7f1db9689a58>,\n",
       " <keras.layers.core.Dense at 0x7f1db9b74710>,\n",
       " <keras.layers.core.Dense at 0x7f1db9b39630>]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ae.layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
