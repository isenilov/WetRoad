{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Autoencoder experiment\n",
    "\n",
    "1. Build an autoencoder and train it on train data (MIT's)\n",
    "2. Save the encoded train data (MIT's)\n",
    "3. Take encoder part from the AE and encode youtube data (the data that will work as \"augmenter\")\n",
    "4. Apply some clustering or other algorithm to find similar segments in both encoded datasets (using some MIT videos as seeds)\n",
    "5. Train NN on the MIT and YT data (those segments that are similar to MIT)\n",
    "6. Validate the classifier (so you may need to leave some of the MIT and YT data out of the training)\n",
    "7. Test on our own data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/media/adilkhan/HDD/Anaconda3/lib/python3.6/site-packages/librosa/filters.py:261: UserWarning: Empty filters detected in mel frequency basis. Some channels will produce empty responses. Try increasing your sampling rate (and fmax) or reducing n_mels.\n",
      "  warnings.warn('Empty filters detected in mel frequency basis. '\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-71-68899a5b1a77>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     48\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mfeatures\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     49\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 50\u001b[0;31m \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mextract_features\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"yt_data/wet01.wav\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"yt_data/dry01.wav\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     51\u001b[0m \u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_test\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_test_split\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.33\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrandom_state\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-71-68899a5b1a77>\u001b[0m in \u001b[0;36mextract_features\u001b[0;34m(file_wet, file_dry)\u001b[0m\n\u001b[1;32m     37\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mfeatures\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     38\u001b[0m     \u001b[0mfeatures_wet\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mextract\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile_wet\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 39\u001b[0;31m     \u001b[0mfeatures_dry\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mextract\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile_dry\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     40\u001b[0m     \u001b[0mlabels_wet\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mones\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfeatures_wet\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     41\u001b[0m     \u001b[0mlabels_dry\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzeros\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfeatures_dry\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-71-68899a5b1a77>\u001b[0m in \u001b[0;36mextract\u001b[0;34m(filename)\u001b[0m\n\u001b[1;32m     21\u001b[0m                                    \u001b[0mn_fft\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnfft\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m                                    \u001b[0mhop_length\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mround\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnfft\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 23\u001b[0;31m                                    fmax=8000)).flatten())\n\u001b[0m\u001b[1;32m     24\u001b[0m     \u001b[0mfeat\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfeat\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mfeat\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/media/adilkhan/HDD/Anaconda3/lib/python3.6/site-packages/librosa/feature/spectral.py\u001b[0m in \u001b[0;36mmfcc\u001b[0;34m(y, sr, S, n_mfcc, **kwargs)\u001b[0m\n\u001b[1;32m   1297\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1298\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mS\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1299\u001b[0;31m         \u001b[0mS\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpower_to_db\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmelspectrogram\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msr\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1300\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1301\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilters\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdct\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn_mfcc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mS\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mS\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/media/adilkhan/HDD/Anaconda3/lib/python3.6/site-packages/librosa/core/spectrum.py\u001b[0m in \u001b[0;36mpower_to_db\u001b[0;34m(S, ref, amin, top_db, ref_power)\u001b[0m\n\u001b[1;32m    734\u001b[0m         \u001b[0mref_value\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mabs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mref\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    735\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 736\u001b[0;31m     \u001b[0mlog_spec\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m10.0\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlog10\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmaximum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mamin\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmagnitude\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    737\u001b[0m     \u001b[0mlog_spec\u001b[0m \u001b[0;34m-=\u001b[0m \u001b[0;36m10.0\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlog10\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmaximum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mamin\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mref_value\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    738\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pickle\n",
    "from scipy.io import wavfile\n",
    "import numpy as np\n",
    "from librosa import feature\n",
    "from keras.utils import to_categorical\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import scale\n",
    "\n",
    "\n",
    "def extract(filename):\n",
    "    rate, frames = wavfile.read(filename)\n",
    "    window = 4096\n",
    "    nfft = 64\n",
    "    feat = []\n",
    "    # len(frames)-window\n",
    "    for i in range(0, len(frames) - window, window):\n",
    "        # feat.append(np.array(frames[i:i + window]))\n",
    "        feat.append(np.array(feature.mfcc(frames[i:i + window - 1],\n",
    "                                   sr=rate,\n",
    "                                   n_fft=nfft,\n",
    "                                   hop_length=round(nfft / 2),\n",
    "                                   fmax=8000)).flatten())\n",
    "    feat = np.stack(feat)\n",
    "    return feat\n",
    "    \n",
    "def extract_features(file_wet, file_dry):\n",
    "    to_replace =\"\\\\/\"\n",
    "    for char in to_replace:\n",
    "        fw = file_wet.replace(char, \"_\")\n",
    "        fd = file_dry.replace(char, \"_\")\n",
    "    pickle_file = fw + \"-\" + fd + \".pkl\"\n",
    "    if os.path.exists(pickle_file):\n",
    "        print(\"Using pickle file\", pickle_file)\n",
    "        with open(pickle_file, \"rb\") as f:\n",
    "            features, labels = pickle.load(f)\n",
    "        return features, labels\n",
    "    features_wet = extract(file_wet)\n",
    "    features_dry = extract(file_dry)\n",
    "    labels_wet = np.ones(features_wet.shape[0])\n",
    "    labels_dry = np.zeros(features_dry.shape[0])\n",
    "    features = np.concatenate((features_wet, features_dry))\n",
    "    labels = np.concatenate((labels_wet, labels_dry))\n",
    "    labels = to_categorical(labels)\n",
    "    features = scale(features)\n",
    "    with open(pickle_file, \"wb\") as f:\n",
    "        pickle.dump((features, labels), f, protocol=4)\n",
    "    return features, labels\n",
    "    \n",
    "X, y = extract_features(\"yt_data/wet01.wav\", \"yt_data/dry01.wav\")\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.,  1.],\n",
       "       [ 1.,  0.],\n",
       "       [ 1.,  0.],\n",
       "       ..., \n",
       "       [ 1.,  0.],\n",
       "       [ 1.,  0.],\n",
       "       [ 0.,  1.]])"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_15 (InputLayer)        (None, 2560)              0         \n",
      "_________________________________________________________________\n",
      "dense_117 (Dense)            (None, 1000)              2561000   \n",
      "_________________________________________________________________\n",
      "dense_118 (Dense)            (None, 500)               500500    \n",
      "_________________________________________________________________\n",
      "dense_119 (Dense)            (None, 200)               100200    \n",
      "_________________________________________________________________\n",
      "dense_120 (Dense)            (None, 500)               100500    \n",
      "_________________________________________________________________\n",
      "dense_121 (Dense)            (None, 1000)              501000    \n",
      "_________________________________________________________________\n",
      "dense_122 (Dense)            (None, 2560)              2562560   \n",
      "=================================================================\n",
      "Total params: 6,325,760\n",
      "Trainable params: 6,325,760\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "from keras.layers import Input, Dense\n",
    "from keras.models import Model\n",
    "\n",
    "# ae = models.Sequential()\n",
    "#encoder\n",
    "# d1 = Dense(1000, activation='tanh',input_shape=(X_train.shape[1:]))\n",
    "# ae.add(d1)\n",
    "# ae.add(BatchNormalization())\n",
    "# #ae.add(Dropout(0.8))\n",
    "# d2 = Dense(500, activation='tanh')\n",
    "# ae.add(d2)\n",
    "# #ae.add(Dropout(0.8))\n",
    "# #decoder\n",
    "# ae.add(Dense(d2.input_shape[1], activation='tanh'))\n",
    "# ae.add(Dense(d1.input_shape[1], activation='tanh'))\n",
    "\n",
    "inp = Input(shape=(X_train.shape[1:]))\n",
    "encoded = Dense(1000, activation='relu')(inp)\n",
    "encoded = Dense(500, activation='relu')(encoded)\n",
    "encoded = Dense(200, activation='relu')(encoded)\n",
    "\n",
    "decoded = Dense(500, activation='relu')(encoded)\n",
    "decoded = Dense(1000, activation='relu')(decoded)\n",
    "decoded = Dense(X_train.shape[1], activation='sigmoid')(decoded)\n",
    "ae = Model(inp, decoded)\n",
    "ae.compile(loss='mse',\n",
    "optimizer='adadelta', metrics=['accuracy'])\n",
    "ae.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 467834 samples, validate on 230427 samples\n",
      "Epoch 1/100\n",
      "467834/467834 [==============================] - 35s - loss: 0.9355 - acc: 1.3466e-04 - val_loss: 0.9144 - val_acc: 2.8642e-04\n",
      "Epoch 2/100\n",
      "467834/467834 [==============================] - 35s - loss: 0.9054 - acc: 4.0185e-04 - val_loss: 0.8995 - val_acc: 6.0323e-04\n",
      "Epoch 3/100\n",
      "467834/467834 [==============================] - 35s - loss: 0.8943 - acc: 0.0010 - val_loss: 0.8885 - val_acc: 0.0018\n",
      "Epoch 4/100\n",
      "467834/467834 [==============================] - 34s - loss: 0.8817 - acc: 0.0033 - val_loss: 0.8745 - val_acc: 0.0047\n",
      "Epoch 5/100\n",
      "467834/467834 [==============================] - 35s - loss: 0.8682 - acc: 0.0079 - val_loss: 0.8618 - val_acc: 0.0110\n",
      "Epoch 6/100\n",
      "467834/467834 [==============================] - 34s - loss: 0.8570 - acc: 0.0141 - val_loss: 0.8527 - val_acc: 0.0178\n",
      "Epoch 7/100\n",
      "467834/467834 [==============================] - 34s - loss: 0.8485 - acc: 0.0207 - val_loss: 0.8451 - val_acc: 0.0235\n",
      "Epoch 8/100\n",
      "467834/467834 [==============================] - 34s - loss: 0.8420 - acc: 0.0265 - val_loss: 0.8396 - val_acc: 0.0288\n",
      "Epoch 9/100\n",
      "467834/467834 [==============================] - 34s - loss: 0.8369 - acc: 0.0309 - val_loss: 0.8357 - val_acc: 0.0329\n",
      "Epoch 10/100\n",
      "467834/467834 [==============================] - 36s - loss: 0.8329 - acc: 0.0351 - val_loss: 0.8320 - val_acc: 0.0367\n",
      "Epoch 11/100\n",
      "467834/467834 [==============================] - 34s - loss: 0.8296 - acc: 0.0380 - val_loss: 0.8287 - val_acc: 0.0403\n",
      "Epoch 12/100\n",
      "467834/467834 [==============================] - 34s - loss: 0.8270 - acc: 0.0407 - val_loss: 0.8264 - val_acc: 0.0426\n",
      "Epoch 13/100\n",
      "467834/467834 [==============================] - 34s - loss: 0.8250 - acc: 0.0427 - val_loss: 0.8243 - val_acc: 0.0445\n",
      "Epoch 14/100\n",
      "467834/467834 [==============================] - 34s - loss: 0.8233 - acc: 0.0450 - val_loss: 0.8235 - val_acc: 0.0480\n",
      "Epoch 15/100\n",
      "467834/467834 [==============================] - 35s - loss: 0.8219 - acc: 0.0465 - val_loss: 0.8221 - val_acc: 0.0492\n",
      "Epoch 16/100\n",
      "467834/467834 [==============================] - 35s - loss: 0.8208 - acc: 0.0478 - val_loss: 0.8204 - val_acc: 0.0488\n",
      "Epoch 17/100\n",
      "467834/467834 [==============================] - 35s - loss: 0.8199 - acc: 0.0483 - val_loss: 0.8212 - val_acc: 0.0495\n",
      "Epoch 18/100\n",
      "467834/467834 [==============================] - 35s - loss: 0.8193 - acc: 0.0491 - val_loss: 0.8195 - val_acc: 0.0500\n",
      "Epoch 19/100\n",
      "467834/467834 [==============================] - 35s - loss: 0.8187 - acc: 0.0498 - val_loss: 0.8201 - val_acc: 0.0497\n",
      "Epoch 20/100\n",
      "467834/467834 [==============================] - 35s - loss: 0.8181 - acc: 0.0504 - val_loss: 0.8182 - val_acc: 0.0512\n",
      "Epoch 21/100\n",
      "467834/467834 [==============================] - 35s - loss: 0.8177 - acc: 0.0506 - val_loss: 0.8182 - val_acc: 0.0527\n",
      "Epoch 22/100\n",
      "467834/467834 [==============================] - 35s - loss: 0.8172 - acc: 0.0510 - val_loss: 0.8185 - val_acc: 0.0520\n",
      "Epoch 23/100\n",
      "467834/467834 [==============================] - 35s - loss: 0.8169 - acc: 0.0515 - val_loss: 0.8178 - val_acc: 0.0522\n",
      "Epoch 24/100\n",
      "467834/467834 [==============================] - 35s - loss: 0.8165 - acc: 0.0515 - val_loss: 0.8172 - val_acc: 0.0512\n",
      "Epoch 25/100\n",
      "467834/467834 [==============================] - 35s - loss: 0.8162 - acc: 0.0519 - val_loss: 0.8182 - val_acc: 0.0519\n",
      "Epoch 26/100\n",
      "467834/467834 [==============================] - 34s - loss: 0.8159 - acc: 0.0522 - val_loss: 0.8169 - val_acc: 0.0530\n",
      "Epoch 27/100\n",
      "467834/467834 [==============================] - 34s - loss: 0.8155 - acc: 0.0520 - val_loss: 0.8166 - val_acc: 0.0531\n",
      "Epoch 28/100\n",
      "467834/467834 [==============================] - 35s - loss: 0.8152 - acc: 0.0520 - val_loss: 0.8152 - val_acc: 0.0533\n",
      "Epoch 29/100\n",
      "467834/467834 [==============================] - 35s - loss: 0.8150 - acc: 0.0524 - val_loss: 0.8155 - val_acc: 0.0540\n",
      "Epoch 30/100\n",
      "467834/467834 [==============================] - 35s - loss: 0.8148 - acc: 0.0528 - val_loss: 0.8152 - val_acc: 0.0533\n",
      "Epoch 31/100\n",
      "467834/467834 [==============================] - 35s - loss: 0.8145 - acc: 0.0528 - val_loss: 0.8159 - val_acc: 0.0542\n",
      "Epoch 32/100\n",
      "467834/467834 [==============================] - 35s - loss: 0.8143 - acc: 0.0531 - val_loss: 0.8153 - val_acc: 0.0529\n",
      "Epoch 33/100\n",
      "467834/467834 [==============================] - 35s - loss: 0.8140 - acc: 0.0531 - val_loss: 0.8144 - val_acc: 0.0550\n",
      "Epoch 34/100\n",
      "467834/467834 [==============================] - 37s - loss: 0.8137 - acc: 0.0525 - val_loss: 0.8149 - val_acc: 0.0541\n",
      "Epoch 35/100\n",
      "467834/467834 [==============================] - 34s - loss: 0.8135 - acc: 0.0530 - val_loss: 0.8144 - val_acc: 0.0535\n",
      "Epoch 36/100\n",
      "467834/467834 [==============================] - 35s - loss: 0.8133 - acc: 0.0532 - val_loss: 0.8146 - val_acc: 0.0547\n",
      "Epoch 37/100\n",
      "467834/467834 [==============================] - 34s - loss: 0.8131 - acc: 0.0533 - val_loss: 0.8138 - val_acc: 0.0535\n",
      "Epoch 38/100\n",
      "467834/467834 [==============================] - 34s - loss: 0.8129 - acc: 0.0533 - val_loss: 0.8143 - val_acc: 0.0543\n",
      "Epoch 39/100\n",
      "467834/467834 [==============================] - 34s - loss: 0.8128 - acc: 0.0534 - val_loss: 0.8141 - val_acc: 0.0543\n",
      "Epoch 40/100\n",
      "467834/467834 [==============================] - 35s - loss: 0.8127 - acc: 0.0534 - val_loss: 0.8142 - val_acc: 0.0553\n",
      "Epoch 41/100\n",
      "467834/467834 [==============================] - 34s - loss: 0.8125 - acc: 0.0537 - val_loss: 0.8145 - val_acc: 0.0525\n",
      "Epoch 42/100\n",
      "467834/467834 [==============================] - 35s - loss: 0.8124 - acc: 0.0536 - val_loss: 0.8134 - val_acc: 0.0548\n",
      "Epoch 43/100\n",
      "467834/467834 [==============================] - 35s - loss: 0.8122 - acc: 0.0538 - val_loss: 0.8132 - val_acc: 0.0530\n",
      "Epoch 44/100\n",
      "467834/467834 [==============================] - 35s - loss: 0.8120 - acc: 0.0537 - val_loss: 0.8136 - val_acc: 0.0541\n",
      "Epoch 45/100\n",
      "467834/467834 [==============================] - 35s - loss: 0.8119 - acc: 0.0537 - val_loss: 0.8131 - val_acc: 0.0537\n",
      "Epoch 46/100\n",
      "467834/467834 [==============================] - 34s - loss: 0.8117 - acc: 0.0540 - val_loss: 0.8129 - val_acc: 0.0554\n",
      "Epoch 47/100\n",
      "467834/467834 [==============================] - 34s - loss: 0.8116 - acc: 0.0540 - val_loss: 0.8131 - val_acc: 0.0546\n",
      "Epoch 48/100\n",
      "467834/467834 [==============================] - 34s - loss: 0.8115 - acc: 0.0543 - val_loss: 0.8128 - val_acc: 0.0544\n",
      "Epoch 49/100\n",
      "467834/467834 [==============================] - 34s - loss: 0.8114 - acc: 0.0541 - val_loss: 0.8136 - val_acc: 0.0543\n",
      "Epoch 50/100\n",
      "467834/467834 [==============================] - 35s - loss: 0.8113 - acc: 0.0542 - val_loss: 0.8131 - val_acc: 0.0544\n",
      "Epoch 51/100\n",
      "467834/467834 [==============================] - 35s - loss: 0.8111 - acc: 0.0543 - val_loss: 0.8124 - val_acc: 0.0554\n",
      "Epoch 52/100\n",
      "467834/467834 [==============================] - 34s - loss: 0.8110 - acc: 0.0545 - val_loss: 0.8125 - val_acc: 0.0547\n",
      "Epoch 53/100\n",
      "467834/467834 [==============================] - 35s - loss: 0.8109 - acc: 0.0546 - val_loss: 0.8127 - val_acc: 0.0552\n",
      "Epoch 54/100\n",
      "467834/467834 [==============================] - 35s - loss: 0.8108 - acc: 0.0545 - val_loss: 0.8124 - val_acc: 0.0547\n",
      "Epoch 55/100\n",
      "467834/467834 [==============================] - 34s - loss: 0.8107 - acc: 0.0546 - val_loss: 0.8123 - val_acc: 0.0549\n",
      "Epoch 56/100\n",
      "467834/467834 [==============================] - 34s - loss: 0.8106 - acc: 0.0544 - val_loss: 0.8119 - val_acc: 0.0548\n",
      "Epoch 57/100\n",
      "467834/467834 [==============================] - 35s - loss: 0.8105 - acc: 0.0545 - val_loss: 0.8113 - val_acc: 0.0550\n",
      "Epoch 58/100\n",
      "467834/467834 [==============================] - 35s - loss: 0.8104 - acc: 0.0546 - val_loss: 0.8123 - val_acc: 0.0549\n",
      "Epoch 59/100\n",
      "467834/467834 [==============================] - 34s - loss: 0.8103 - acc: 0.0543 - val_loss: 0.8119 - val_acc: 0.0551\n",
      "Epoch 60/100\n",
      "467834/467834 [==============================] - 34s - loss: 0.8102 - acc: 0.0546 - val_loss: 0.8116 - val_acc: 0.0556\n",
      "Epoch 61/100\n",
      "467834/467834 [==============================] - 34s - loss: 0.8102 - acc: 0.0545 - val_loss: 0.8116 - val_acc: 0.0563\n",
      "Epoch 62/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "467834/467834 [==============================] - 34s - loss: 0.8101 - acc: 0.0548 - val_loss: 0.8117 - val_acc: 0.0563\n",
      "Epoch 63/100\n",
      "467834/467834 [==============================] - 34s - loss: 0.8100 - acc: 0.0545 - val_loss: 0.8113 - val_acc: 0.0561\n",
      "Epoch 64/100\n",
      "467834/467834 [==============================] - 34s - loss: 0.8099 - acc: 0.0546 - val_loss: 0.8111 - val_acc: 0.0550\n",
      "Epoch 65/100\n",
      "467834/467834 [==============================] - 35s - loss: 0.8099 - acc: 0.0544 - val_loss: 0.8116 - val_acc: 0.0557\n",
      "Epoch 66/100\n",
      "467834/467834 [==============================] - 34s - loss: 0.8098 - acc: 0.0548 - val_loss: 0.8118 - val_acc: 0.0565\n",
      "Epoch 67/100\n",
      "467834/467834 [==============================] - 34s - loss: 0.8097 - acc: 0.0547 - val_loss: 0.8114 - val_acc: 0.0570\n",
      "Epoch 68/100\n",
      "467834/467834 [==============================] - 34s - loss: 0.8096 - acc: 0.0546 - val_loss: 0.8112 - val_acc: 0.0546\n",
      "Epoch 69/100\n",
      "467834/467834 [==============================] - 35s - loss: 0.8096 - acc: 0.0546 - val_loss: 0.8110 - val_acc: 0.0549\n",
      "Epoch 70/100\n",
      "467834/467834 [==============================] - 34s - loss: 0.8095 - acc: 0.0546 - val_loss: 0.8112 - val_acc: 0.0551\n",
      "Epoch 71/100\n",
      "467834/467834 [==============================] - 34s - loss: 0.8094 - acc: 0.0548 - val_loss: 0.8104 - val_acc: 0.0568\n",
      "Epoch 72/100\n",
      "467834/467834 [==============================] - 34s - loss: 0.8093 - acc: 0.0548 - val_loss: 0.8112 - val_acc: 0.0562\n",
      "Epoch 73/100\n",
      "467834/467834 [==============================] - 34s - loss: 0.8093 - acc: 0.0549 - val_loss: 0.8104 - val_acc: 0.0555\n",
      "Epoch 74/100\n",
      "467834/467834 [==============================] - 34s - loss: 0.8092 - acc: 0.0551 - val_loss: 0.8110 - val_acc: 0.0559\n",
      "Epoch 75/100\n",
      "467834/467834 [==============================] - 34s - loss: 0.8091 - acc: 0.0548 - val_loss: 0.8106 - val_acc: 0.0552\n",
      "Epoch 76/100\n",
      "467834/467834 [==============================] - 34s - loss: 0.8090 - acc: 0.0547 - val_loss: 0.8109 - val_acc: 0.0549\n",
      "Epoch 77/100\n",
      "467834/467834 [==============================] - 34s - loss: 0.8089 - acc: 0.0548 - val_loss: 0.8106 - val_acc: 0.0556\n",
      "Epoch 78/100\n",
      "467834/467834 [==============================] - 34s - loss: 0.8088 - acc: 0.0544 - val_loss: 0.8103 - val_acc: 0.0563\n",
      "Epoch 79/100\n",
      "467834/467834 [==============================] - 34s - loss: 0.8088 - acc: 0.0548 - val_loss: 0.8106 - val_acc: 0.0569\n",
      "Epoch 80/100\n",
      "467834/467834 [==============================] - 34s - loss: 0.8087 - acc: 0.0547 - val_loss: 0.8104 - val_acc: 0.0554\n",
      "Epoch 81/100\n",
      "467834/467834 [==============================] - 35s - loss: 0.8085 - acc: 0.0549 - val_loss: 0.8102 - val_acc: 0.0555\n",
      "Epoch 82/100\n",
      "467834/467834 [==============================] - 34s - loss: 0.8084 - acc: 0.0550 - val_loss: 0.8098 - val_acc: 0.0562\n",
      "Epoch 83/100\n",
      "467834/467834 [==============================] - 34s - loss: 0.8084 - acc: 0.0549 - val_loss: 0.8102 - val_acc: 0.0555\n",
      "Epoch 84/100\n",
      "467834/467834 [==============================] - 35s - loss: 0.8083 - acc: 0.0550 - val_loss: 0.8100 - val_acc: 0.0546\n",
      "Epoch 85/100\n",
      "467834/467834 [==============================] - 34s - loss: 0.8082 - acc: 0.0547 - val_loss: 0.8103 - val_acc: 0.0560\n",
      "Epoch 86/100\n",
      "467834/467834 [==============================] - 34s - loss: 0.8082 - acc: 0.0548 - val_loss: 0.8100 - val_acc: 0.0557\n",
      "Epoch 87/100\n",
      "467834/467834 [==============================] - 35s - loss: 0.8081 - acc: 0.0552 - val_loss: 0.8102 - val_acc: 0.0559\n",
      "Epoch 88/100\n",
      "467834/467834 [==============================] - 34s - loss: 0.8080 - acc: 0.0549 - val_loss: 0.8099 - val_acc: 0.0563\n",
      "Epoch 89/100\n",
      "467834/467834 [==============================] - 34s - loss: 0.8079 - acc: 0.0548 - val_loss: 0.8095 - val_acc: 0.0553\n",
      "Epoch 90/100\n",
      "467834/467834 [==============================] - 36s - loss: 0.8077 - acc: 0.0543 - val_loss: 0.8097 - val_acc: 0.0555\n",
      "Epoch 91/100\n",
      "467834/467834 [==============================] - 34s - loss: 0.8077 - acc: 0.0545 - val_loss: 0.8094 - val_acc: 0.0564\n",
      "Epoch 92/100\n",
      "467834/467834 [==============================] - 35s - loss: 0.8076 - acc: 0.0545 - val_loss: 0.8089 - val_acc: 0.0567\n",
      "Epoch 93/100\n",
      "467834/467834 [==============================] - 34s - loss: 0.8076 - acc: 0.0550 - val_loss: 0.8095 - val_acc: 0.0566\n",
      "Epoch 94/100\n",
      "467834/467834 [==============================] - 34s - loss: 0.8075 - acc: 0.0549 - val_loss: 0.8096 - val_acc: 0.0563\n",
      "Epoch 95/100\n",
      "467834/467834 [==============================] - 34s - loss: 0.8074 - acc: 0.0549 - val_loss: 0.8087 - val_acc: 0.0559\n",
      "Epoch 96/100\n",
      "467834/467834 [==============================] - 33s - loss: 0.8074 - acc: 0.0552 - val_loss: 0.8089 - val_acc: 0.0561\n",
      "Epoch 97/100\n",
      "467834/467834 [==============================] - 34s - loss: 0.8073 - acc: 0.0547 - val_loss: 0.8089 - val_acc: 0.0555\n",
      "Epoch 98/100\n",
      "467834/467834 [==============================] - 34s - loss: 0.8072 - acc: 0.0548 - val_loss: 0.8089 - val_acc: 0.0571\n",
      "Epoch 99/100\n",
      "467834/467834 [==============================] - 34s - loss: 0.8072 - acc: 0.0548 - val_loss: 0.8087 - val_acc: 0.0569\n",
      "Epoch 100/100\n",
      "467834/467834 [==============================] - 33s - loss: 0.8071 - acc: 0.0550 - val_loss: 0.8086 - val_acc: 0.0557\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f61c3d0e128>"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ae.fit(X_train, X_train, epochs=100,\n",
    "          batch_size=128,\n",
    "          validation_data=(X_test, X_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
