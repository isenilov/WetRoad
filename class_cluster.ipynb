{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from scipy.io import wavfile\n",
    "import numpy as np\n",
    "from scipy import signal\n",
    "# from librosa import feature, effects, load\n",
    "from sklearn.preprocessing import minmax_scale\n",
    "import os.path\n",
    "import pickle\n",
    "\n",
    "\n",
    "def extract(wav_file, nfft=64, window_length=0.1, mel=False, flatten=True, augment=False, noise=False):\n",
    "    rate, frames = wavfile.read(wav_file)\n",
    "    window = 16384  # round(window_length * rate)\n",
    "    feat = []\n",
    "\n",
    "    for i in range(0, len(frames)-window, int(window/2)):\n",
    "        if mel:\n",
    "            pxx = np.array(feature.mfcc(frames[i:i + window - 1],\n",
    "                               sr=rate,\n",
    "                               n_fft=nfft,\n",
    "                               hop_length=round(nfft / 2),\n",
    "                               fmax=8000))\n",
    "        else:\n",
    "            pxx = np.array(frames[i:i + window])\n",
    "        if flatten:\n",
    "            feat.append(pxx.flatten())\n",
    "        else:\n",
    "            feat.append(pxx)\n",
    "            '''TODO: experiments with augmentation'''\n",
    "            if augment:\n",
    "                feat.append(effects.pitch_shift(pxx, rate, n_steps=4.0))\n",
    "                # feat.append(effects.pitch_shift(pxx, rate, n_steps=8.0))\n",
    "                feat.append(effects.pitch_shift(pxx, rate, n_steps=-4.0))\n",
    "                # feat.append(effects.pitch_shift(pxx, rate, n_steps=-8.0))\n",
    "            if noise:\n",
    "                feat.append(pxx + np.random.normal(0, 1, len(pxx)))\n",
    "                feat.append(pxx * np.random.normal(1, 0.1, len(pxx)))\n",
    "    return np.stack(feat)\n",
    "\n",
    "\n",
    "def extract_features(file_wet, file_dry, mel=False, flatten=True, scaling=False, categorical=True, augment=False, noise=False):\n",
    "    to_replace =\"\\\\/\"\n",
    "    for char in to_replace:\n",
    "        fw = file_wet.replace(char, \"_\")\n",
    "        fd = file_dry.replace(char, \"_\")\n",
    "    pickle_file = fw + \"-\" + fd + \".pkl\"\n",
    "    if os.path.exists(pickle_file):\n",
    "        print(\"Using pickle file\", pickle_file)\n",
    "        with open(pickle_file, \"rb\") as f:\n",
    "            features, labels = pickle.load(f)\n",
    "        return features, labels\n",
    "    features_wet = extract(file_wet, mel=mel, flatten=flatten, augment=augment, noise=noise)\n",
    "    features_dry = extract(file_dry, mel=mel, flatten=flatten, augment=augment, noise=noise)\n",
    "    print(features_dry, features_dry.shape)\n",
    "    labels_wet = np.ones(features_wet.shape[0])\n",
    "    labels_dry = np.zeros(features_dry.shape[0])\n",
    "    features = np.concatenate((features_wet, features_dry))\n",
    "    labels = np.concatenate((labels_wet, labels_dry))\n",
    "    if categorical:\n",
    "        from keras.utils import to_categorical\n",
    "        labels = to_categorical(labels, 2)\n",
    "    if scaling and flatten:\n",
    "        features = minmax_scale(features)\n",
    "    with open(pickle_file, \"wb\") as f:\n",
    "        pickle.dump((features, labels), f, protocol=4)\n",
    "    return features, labels\n",
    "\n",
    "\n",
    "def get_last(path, type):\n",
    "    import glob\n",
    "    if type == \"weights\":\n",
    "        list = sorted(glob.glob(path + \"*.h5\"))\n",
    "    if type == \"model\":\n",
    "        list = sorted(glob.glob(path + \"*.yaml\"))\n",
    "    if len(list) > 0:\n",
    "        return max(listt)\n",
    "    return None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from feature_extraction import extract_features, get_last\n",
    "from sklearn.metrics import recall_score, accuracy_score\n",
    "from sklearn.utils import shuffle\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from time import time\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Conv1D, MaxPooling1D, GlobalAveragePooling1D, Dropout, Dense, Flatten, LSTM\n",
    "from keras.layers.wrappers import Bidirectional, TimeDistributed\n",
    "from keras.callbacks import TensorBoard, EarlyStopping, ModelCheckpoint, Callback\n",
    "from keras import optimizers, regularizers\n",
    "from keras.utils import to_categorical\n",
    "from datetime import datetime\n",
    "import os\n",
    "\n",
    "dt = datetime.now().strftime(\"%d-%m-%Y.%H-%M\")\n",
    "\n",
    "class TestCallback(Callback):\n",
    "    def __init__(self, test_data, number):\n",
    "        self.test_data = test_data\n",
    "        self.number = number\n",
    "\n",
    "    def on_epoch_end(self, epoch, logs={}):\n",
    "        x, y = self.test_data\n",
    "        loss, acc = self.model.evaluate(x, y, verbose=0)\n",
    "        log_filename = \"models/cnn/log.\" + dt + \".csv\"\n",
    "        with open(log_filename, \"a\") as log:\n",
    "            log.write(\"{},{},{},{},{}\\n\".format(self.number, epoch, loss, acc, logs[\"acc\"]))\n",
    "\n",
    "def def_model_cnn_blstm(input_shape):\n",
    "    model = Sequential()\n",
    "    model.add(TimeDistributed(Conv1D(filters=64, kernel_size=64, strides=1, padding=\"same\", activation='tanh'), input_shape=input_shape))\n",
    "    model.add(TimeDistributed(MaxPooling1D(2)))\n",
    "    model.add(TimeDistributed(Dropout(0.4)))\n",
    "    # model.add(TimeDistributed(Conv1D(32, 32, activation='relu')))\n",
    "    # model.add(TimeDistributed(MaxPooling1D(4)))\n",
    "    # model.add(TimeDistributed(Dropout(0.3)))\n",
    "    model.add(TimeDistributed(Conv1D(64, 64, padding=\"same\", activation='tanh')))\n",
    "    model.add(TimeDistributed(MaxPooling1D(2)))\n",
    "    model.add(TimeDistributed(Dropout(0.4)))\n",
    "    model.add(TimeDistributed(Conv1D(64, 64, padding=\"same\", activation='tanh')))\n",
    "    model.add(TimeDistributed(MaxPooling1D(2)))\n",
    "    model.add(TimeDistributed(Dropout(0.4)))\n",
    "    # model.add(TimeDistributed(Conv1D(64, 16, activation='relu')))\n",
    "    # model.add(TimeDistributed(MaxPooling1D(4)))\n",
    "    # model.add(TimeDistributed(Dropout(0.4)))\n",
    "    model.add(TimeDistributed(Conv1D(128, 64, padding=\"same\", activation='tanh')))\n",
    "    model.add(TimeDistributed(MaxPooling1D(2)))\n",
    "    model.add(TimeDistributed(Dropout(0.4)))\n",
    "    model.add(TimeDistributed(Conv1D(128, 64, padding=\"same\", activation='tanh')))\n",
    "    model.add(TimeDistributed(MaxPooling1D(2)))\n",
    "    # model.add(TimeDistributed(Dropout(0.4)))\n",
    "    model.add(TimeDistributed(Conv1D(128, 64, padding=\"same\", activation='tanh')))\n",
    "    model.add(TimeDistributed(MaxPooling1D(2)))\n",
    "    # model.add(TimeDistributed(Dropout(0.4)))\n",
    "    model.add(TimeDistributed(Conv1D(256, 64, padding=\"same\", activation='tanh')))\n",
    "    model.add(TimeDistributed(MaxPooling1D(2)))\n",
    "    # model.add(TimeDistributed(Dropout(0.4)))\n",
    "    model.add(TimeDistributed(Conv1D(256, 64, padding=\"same\", activation='tanh')))\n",
    "    model.add(TimeDistributed(GlobalAveragePooling1D()))\n",
    "\n",
    "    # model.add(TimeDistributed(Dense(128, activation='relu')))\n",
    "    # model.add(TimeDistributed(Dropout(0.5)))\n",
    "    # model.add(Bidirectional(LSTM(256, return_sequences=True)))\n",
    "    # model.add(TimeDistributed(Dropout(0.5)))\n",
    "    # model.add(Bidirectional(LSTM(256, return_sequences=True)))\n",
    "    model.add(Bidirectional(LSTM(256)))\n",
    "    model.add(Dense(2, activation='softmax'))\n",
    "    model.compile(loss='categorical_crossentropy',\n",
    "                  optimizer='adam',\n",
    "                  metrics=['accuracy'])\n",
    "    model.summary()\n",
    "    return model\n",
    "\n",
    "\n",
    "def train():\n",
    "    X_train, X_1, X_2, X_3, y_train, y_1, y_2, y_3 = ex_feat()\n",
    "    start = time()\n",
    "    print(\"\\nTraining model...\")\n",
    "    model = def_model_cnn_blstm(X_train.shape[1:])\n",
    "    weights = get_last(\"models/cnn/\", \"weights\")\n",
    "    if weights is not None:\n",
    "        model.load_weights(weights)\n",
    "    print(\"Using weights:\", weights)\n",
    "    print(\"Dataset shape:\", X_train.shape)\n",
    "\n",
    "    # tbCallback = TensorBoard(histogram_freq=1, write_grads=True, write_graph=False)  # Tensorboard callback\n",
    "    # esCallback = EarlyStopping(monitor=\"val_loss\", min_delta=0.01, patience=5, verbose=1)  # early stopping callback\n",
    "    mcCallback = ModelCheckpoint(\"models/cnn/weights.{epoch:02d}-{val_acc:.4f}.h5\", monitor='val_acc', verbose=0,\n",
    "                                 save_best_only=False, save_weights_only=True,\n",
    "                                 mode='auto', period=1)  # saving weights every epoch\n",
    "    testCallback0 = TestCallback((X_train, y_train), 3)\n",
    "    testCallback1 = TestCallback((X_1, y_1), 1)\n",
    "    testCallback2 = TestCallback((X_2, y_2), 2)\n",
    "    testCallback3 = TestCallback((X_3, y_3), 3)\n",
    "\n",
    "\n",
    "    # dt = datetime.now().strftime(\"%d-%m-%Y.%H-%M\")\n",
    "    model_filename = \"models/cnn/model.\" + dt + \".yaml\"\n",
    "    with open(model_filename, \"w\") as model_yaml:\n",
    "        model_yaml.write(model.to_yaml())\n",
    "\n",
    "    model.fit(X_train, y_train, validation_data=(X_1, y_1),\n",
    "              batch_size=128, epochs=75, verbose=1,\n",
    "              callbacks=[mcCallback, testCallback0, testCallback1, testCallback2, testCallback3]) #, esCallback])\n",
    "\n",
    "    weights_filename = \"models/cnn/\" + dt + \".h5\"\n",
    "    model.save_weights(weights_filename)\n",
    "    end = time()\n",
    "    training_time = end - start\n",
    "    print(\"\\nTook %.3f sec.\" % training_time)\n",
    "\n",
    "\n",
    "def ex_feat():\n",
    "    start = time()\n",
    "    print(\"\\nExtracting features...\")\n",
    "    # X_1, y_1 = extract_features(\"dataset/wet1/audio_mono.wav\",\n",
    "    #                             \"dataset/dry1/audio_mono.wav\", flatten=False, scaling=False)\n",
    "    # X_2, y_2 = extract_features(\"dataset/wet2/audio_mono.wav\",\n",
    "    #                             \"dataset/dry2/audio_mono.wav\", flatten=False, scaling=False)\n",
    "    # X_3, y_3 = extract_features(\"dataset/wet3/audio_mono.wav\",\n",
    "    #                             \"dataset/dry3/audio_mono.wav\", flatten=False, scaling=False)\n",
    "    #\n",
    "    # X_train = np.concatenate((X_1, X_2, X_3))\n",
    "    # y_train = np.concatenate((y_1, y_2, y_3))\n",
    "    #\n",
    "    # X_test, y_test = extract_features(\"dataset/wet/chevy_wet.wav\",\n",
    "    #                                   \"dataset/dry/chevy_dry.wav\", flatten=False, scaling=False)\n",
    "    # X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=42)\n",
    "    # X_train, y_train = extract_features(\"dataset/wet/test_wet.wav\", \"dataset/dry/test_dry.wav\",\n",
    "    #                                     mel=False, flatten=False, scaling=True, categorical=True, augment=True)\n",
    "    # X_test, y_test = extract_features(\"dataset/wet/test_wet.wav\", \"dataset/dry/test_dry.wav\",\n",
    "    #                                   mel=False, flatten=False, scaling=True, categorical=True)\n",
    "    # X_val, y_val = extract_features(\"dataset/wet/test_wet.wav\", \"dataset/dry/test_dry.wav\",\n",
    "    #                                 mel=False, flatten=False, scaling=True, categorical=True)\n",
    "    X_train, y_train = extract_features(\"yt_data/wet_0.wav\", \"yt_data/dry_0.wav\",\n",
    "                                        mel=False, flatten=False, scaling=True, categorical=True)\n",
    "    X_1, y_1 = extract_features(\"dataset/wet1/audio_mono.wav\", \"dataset/dry1/audio_mono.wav\",\n",
    "                                      mel=False, flatten=False, scaling=True, categorical=True)\n",
    "    X_2, y_2 = extract_features(\"dataset/wet2/audio_mono.wav\", \"dataset/dry2/audio_mono.wav\",\n",
    "                                    mel=False, flatten=False, scaling=True, categorical=True)\n",
    "    X_3, y_3 = extract_features(\"dataset/wet3/audio_mono.wav\", \"dataset/dry3/audio_mono.wav\",\n",
    "                               mel=False, flatten=False, scaling=True, categorical=True)\n",
    "\n",
    "    X_train = np.expand_dims(X_train, axis=1)\n",
    "    X_1 = np.expand_dims(X_1, axis=1)\n",
    "    X_2 = np.expand_dims(X_2, axis=1)\n",
    "    X_3 = np.expand_dims(X_3, axis=1)\n",
    "\n",
    "    X_train = X_train.reshape((X_train.shape[0], 1, int(X_train.shape[2])))\n",
    "    X_1 = X_1.reshape((X_1.shape[0], 1, int(X_1.shape[2])))\n",
    "    X_2 = X_2.reshape((X_2.shape[0], 1, int(X_2.shape[2])))\n",
    "    X_3 = X_3.reshape((X_3.shape[0], 1, int(X_3.shape[2])))\n",
    "\n",
    "    X_train = np.expand_dims(X_train, axis=3)\n",
    "    X_1 = np.expand_dims(X_1, axis=3)\n",
    "    X_2 = np.expand_dims(X_2, axis=3)\n",
    "    X_3 = np.expand_dims(X_3, axis=3)\n",
    "\n",
    "    end = time()\n",
    "    print(\"Took %.3f sec.\" % (end - start))\n",
    "\n",
    "\n",
    "    return X_train, X_1, X_2, X_3, y_train, y_1, y_2, y_3\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Extracting features...\n",
      "Using pickle file yt_data_wet_0.wav-yt_data_dry_0.wav.pkl\n",
      "Using pickle file dataset_wet1_audio_mono.wav-dataset_dry1_audio_mono.wav.pkl\n",
      "Using pickle file dataset_wet2_audio_mono.wav-dataset_dry2_audio_mono.wav.pkl\n",
      "Using pickle file dataset_wet3_audio_mono.wav-dataset_dry3_audio_mono.wav.pkl\n",
      "Took 17.509 sec.\n",
      "\n",
      "Training model...\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "time_distributed_1 (TimeDist (None, 1, 16384, 64)      4160      \n",
      "_________________________________________________________________\n",
      "time_distributed_2 (TimeDist (None, 1, 8192, 64)       0         \n",
      "_________________________________________________________________\n",
      "time_distributed_3 (TimeDist (None, 1, 8192, 64)       0         \n",
      "_________________________________________________________________\n",
      "time_distributed_4 (TimeDist (None, 1, 8192, 64)       262208    \n",
      "_________________________________________________________________\n",
      "time_distributed_5 (TimeDist (None, 1, 4096, 64)       0         \n",
      "_________________________________________________________________\n",
      "time_distributed_6 (TimeDist (None, 1, 4096, 64)       0         \n",
      "_________________________________________________________________\n",
      "time_distributed_7 (TimeDist (None, 1, 4096, 64)       262208    \n",
      "_________________________________________________________________\n",
      "time_distributed_8 (TimeDist (None, 1, 2048, 64)       0         \n",
      "_________________________________________________________________\n",
      "time_distributed_9 (TimeDist (None, 1, 2048, 64)       0         \n",
      "_________________________________________________________________\n",
      "time_distributed_10 (TimeDis (None, 1, 2048, 128)      524416    \n",
      "_________________________________________________________________\n",
      "time_distributed_11 (TimeDis (None, 1, 1024, 128)      0         \n",
      "_________________________________________________________________\n",
      "time_distributed_12 (TimeDis (None, 1, 1024, 128)      0         \n",
      "_________________________________________________________________\n",
      "time_distributed_13 (TimeDis (None, 1, 1024, 128)      1048704   \n",
      "_________________________________________________________________\n",
      "time_distributed_14 (TimeDis (None, 1, 512, 128)       0         \n",
      "_________________________________________________________________\n",
      "time_distributed_15 (TimeDis (None, 1, 512, 128)       1048704   \n",
      "_________________________________________________________________\n",
      "time_distributed_16 (TimeDis (None, 1, 256, 128)       0         \n",
      "_________________________________________________________________\n",
      "time_distributed_17 (TimeDis (None, 1, 256, 256)       2097408   \n",
      "_________________________________________________________________\n",
      "time_distributed_18 (TimeDis (None, 1, 128, 256)       0         \n",
      "_________________________________________________________________\n",
      "time_distributed_19 (TimeDis (None, 1, 128, 256)       4194560   \n",
      "_________________________________________________________________\n",
      "time_distributed_20 (TimeDis (None, 1, 256)            0         \n",
      "_________________________________________________________________\n",
      "bidirectional_1 (Bidirection (None, 512)               1050624   \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 2)                 1026      \n",
      "=================================================================\n",
      "Total params: 10,494,018\n",
      "Trainable params: 10,494,018\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Using weights: models/cnn/weights.33.h5\n",
      "Dataset shape: (162393, 1, 16384, 1)\n",
      "Train on 162393 samples, validate on 18770 samples\n",
      "Epoch 1/75\n",
      "   256/162393 [..............................] - ETA: 14:22:11 - loss: 0.5850 - acc: 0.7305"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-3-93fd337a0d5c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-2-568b85354b8e>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m()\u001b[0m\n\u001b[1;32m    103\u001b[0m     model.fit(X_train, y_train, validation_data=(X_1, y_1),\n\u001b[1;32m    104\u001b[0m               \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m128\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m75\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 105\u001b[0;31m               callbacks=[mcCallback, testCallback0, testCallback1, testCallback2, testCallback3]) #, esCallback])\n\u001b[0m\u001b[1;32m    106\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    107\u001b[0m     \u001b[0mweights_filename\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"models/cnn/\"\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mdt\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m\".h5\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/media/adilkhan/HDD/Anaconda3/envs/ivan/lib/python3.6/site-packages/keras/models.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, **kwargs)\u001b[0m\n\u001b[1;32m    958\u001b[0m                               \u001b[0minitial_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minitial_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    959\u001b[0m                               \u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 960\u001b[0;31m                               validation_steps=validation_steps)\n\u001b[0m\u001b[1;32m    961\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    962\u001b[0m     def evaluate(self, x, y, batch_size=32, verbose=1,\n",
      "\u001b[0;32m/media/adilkhan/HDD/Anaconda3/envs/ivan/lib/python3.6/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, **kwargs)\u001b[0m\n\u001b[1;32m   1648\u001b[0m                               \u001b[0minitial_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minitial_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1649\u001b[0m                               \u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1650\u001b[0;31m                               validation_steps=validation_steps)\n\u001b[0m\u001b[1;32m   1651\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1652\u001b[0m     def evaluate(self, x=None, y=None,\n",
      "\u001b[0;32m/media/adilkhan/HDD/Anaconda3/envs/ivan/lib/python3.6/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36m_fit_loop\u001b[0;34m(self, f, ins, out_labels, batch_size, epochs, verbose, callbacks, val_f, val_ins, shuffle, callback_metrics, initial_epoch, steps_per_epoch, validation_steps)\u001b[0m\n\u001b[1;32m   1211\u001b[0m                     \u001b[0mbatch_logs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'size'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_ids\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1212\u001b[0m                     \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_batch_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_index\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_logs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1213\u001b[0;31m                     \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1214\u001b[0m                     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1215\u001b[0m                         \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mouts\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/media/adilkhan/HDD/Anaconda3/envs/ivan/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2350\u001b[0m         \u001b[0msession\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_session\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2351\u001b[0m         updated = session.run(fetches=fetches, feed_dict=feed_dict,\n\u001b[0;32m-> 2352\u001b[0;31m                               **self.session_kwargs)\n\u001b[0m\u001b[1;32m   2353\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mupdated\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2354\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/media/adilkhan/HDD/Anaconda3/envs/ivan/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    887\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    888\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 889\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    890\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    891\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/media/adilkhan/HDD/Anaconda3/envs/ivan/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1118\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mfeed_dict_tensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1119\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m-> 1120\u001b[0;31m                              feed_dict_tensor, options, run_metadata)\n\u001b[0m\u001b[1;32m   1121\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1122\u001b[0m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/media/adilkhan/HDD/Anaconda3/envs/ivan/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1315\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1316\u001b[0m       return self._do_call(_run_fn, self._session, feeds, fetches, targets,\n\u001b[0;32m-> 1317\u001b[0;31m                            options, run_metadata)\n\u001b[0m\u001b[1;32m   1318\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1319\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_prun_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeeds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/media/adilkhan/HDD/Anaconda3/envs/ivan/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1321\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1322\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1323\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1324\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1325\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/media/adilkhan/HDD/Anaconda3/envs/ivan/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(session, feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m   1300\u001b[0m           return tf_session.TF_Run(session, options,\n\u001b[1;32m   1301\u001b[0m                                    \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1302\u001b[0;31m                                    status, run_metadata)\n\u001b[0m\u001b[1;32m   1303\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1304\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msession\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
